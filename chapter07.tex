\chapter{A Prototype Tool} \label{chapter: A Prototype Tool}
\section{Introduction}
This chapter discusses the prototype that provides a proof of concept for this research. It includes every module of the prototype discussed and described in this chapter. All architecture designs and specifications are described in detail. Note that not all modules and ideas that are discussed here are implemented, this detail will be mentioned for every module.

This prototype presents an ability of programming using speech and represents code in a compact way. It is written in the Java language and uses different third party services and applications. Every service and application that is used is discussed and described.

\section{Speech to Text}
This is a module which is responsible for the translation from speech to text. The engine that is used for the STT process is the Google Speech V2 server. How to interact with this server is shown in \citet{google15}  .
\subsection{Google Speech V2 Server}
Google Speech V2 server provides STT service with no charge for everyone who willing to use their service.
\remark{In order to use this service you have to be a chromium user (\url{https://www.chromium.org/}) because Google's server requires developer's key}.
\subsubsection{Host}
In order to access this service the host that must be used is \url{https://www.google.com/speech-api/v2/recognize}.
\subsubsection{Request}
The request that must be sent to Google's server is:
\begin{lstlisting}
curl -X POST \
--data-binary @'test.flac' \
--header 'Content-Type: audio/x-flac; rate=44100;' \
'https://www.google.com/speech-api/v2/recognize?output=json&lang=en-us&key=
AIzaSyDT41KV3j_c2OseWNNt4xv79MD9sj9p2j4'
\end{lstlisting}
Explanation about the curl:
\begin{description}
	\item [data-binary] is a path on your local machine to audio file that you want to translate.
	\item [header] is the information about encoding of the audio. In this example is flac with a bit rate of 44,100.
	\item [uri] is the host of the server with concatenation of the output format, language, and developer key. In this example we use json (output=json), English (lang=en-us), and our developer key is AIzaSyDT41KV3j\_c2OseWNNt4xv79MD9sj9p2j4.
\end{description}
\subsubsection{Response}
The response format is JSON. When Google is 100\% confident in its translation, it will return the following object:
\begin{lstlisting}
	{
	   "result":
	   [
	   {
	   		"alternative":
	   		[
	   		{
	   			"transcript":"good morning Google how are you feeling today"
	   		}
	   		],
	   		"final":true
	   	}
	   	],
	   	"result_index":0
	}
\end{lstlisting}
When it's doubtful, it adds a confidence parameter for you. It also seems to add multiple transcripts for some reason.
\begin{lstlisting}
	{
		"result":
		[
		{
			"alternative":
			[
				{
					"transcript":"this is a test",
					"confidence":0.97321892
		        },
		        {
		        	"transcript":"this is a test for"
		        }
	      	],
	      	"final":true
	      }
	      ],
	      "result_index":0
	}
\end{lstlisting}
\subsection{Speech to Text Library}
This library is made by Florian Schulz \cite{getflourish14}, it is used to bring speech recognition to Java applications using WebSocket and Google Speech V2 Server. The new API of Google's server has a limit of 50 requests per day.

This library is an open source with dependency in the Processing library but we removed the dependency in Processing because we would like to allow any Java application to use this library, hence we had to make modifications so it will fit our needs. 
\textit{"Processing is a flexible software sketchbook and a language for learning how to code within the context of the visual arts"} \citet{processing15}. Processing is not part of this project but you can read about it in this \href{https://processing.org}{link}.
The modifications that were made to this library:
\begin{itemize}
	\item The dependency in Processing was removed so the application could run as a simple Java project. 
	\item A new response parser has been developed because the one that was in use didn't know how to parse the new format of Google Speech V2.
	\item Auto Speech Recording had memory issues.
	\item Various redesign modifications.
\end{itemize}
We use this library in the Speech To Text layer. It is shown in \citet{getflourish14} where to get the source code from and how to use its API.
\subsection{Testing the Module}
In order to be sure that we can use this module in our system we performed several tests with several testers. We used the Linked List program that is presented in \autoref{section: Example} to perform the main test. \autoref{tab16} and \autoref{tab17} are the result of the dictation of the Linked List program. Every command was dictated separately, each row contains three columns: Expected Result, is the context that was dictated, Actual Result, is the context that was returned, and Match Score, represents how both contexts are matched. Since programming is not a natural language then we expect mismatches. For example, the user may dictate "create field has next of type boolean". He means that he wants to create a field named hasNext which is of type boolean. The module will return "create field has next of type boolean". We do not expect the speech to text engine to return hasNext, therefore, this conclusion should be considered in the Dictation Parser. A similar issue is shortened words. For example, if the user dictates "create field pos" then the system may return "create field pose". pos is different from pose, since pos is not a legal word in the language.  We expect this mismatch and assume that this will be handled by the user.

We calculated the average matching and the result was 97.5\%. This result is within the accectable limitations for our system. The response time is dependent on the length of the context and how busy the server is. The response time is [0.5, 5] seconds. The response time is within our acceptable range. If we want to upgrade the performance of our system we may consider to upgrading this module. For example, we may exchange it with IBM Watson.


\begin{table}[H]
\centering
\begin{tabular}{|p{6cm}|p{6cm}|p{2cm}|}
\hline
{\bf Expected Result} & {\bf Actual Result} & {\bf Match Score (\%)} \\ \hline
 create class linked list & create class linked list & 100 \\ \hline
 create inner class node & create inner class node & 100 \\ \hline
 create field data of type object & create field data of type object & 100 \\ \hline
 make it public & make it public & 100 \\ \hline
 create public field next of type object & create public field next of type object & 100 \\ \hline
 we are done with class node & we are done with class node & 100 \\ \hline
 create field tail of type node & create field tail of type node & 100 \\ \hline
 create an int field size & create an int field size & 100 \\ \hline
 create method iterator & create method iterator & 100 \\ \hline
 return new llinterrater & return new ll interrater & 100 \\ \hline
 number 2 & number 2 & 100 \\ \hline
 exit method & exit method & 100 \\ \hline
 create inner class LLinterrater & create inner class LLL interrater & 90 \\ \hline
 create a constructor & create a constructor & 100\\ \hline
 assign head to field next node & assign head to field next node & 100 \\ \hline
 assign false to field removeok & assign false to field remove ok & 100 \\ \hline
 assign minus 1 to field postoremove & assign minus 12 field post to remove & 60 \\ \hline
 number 2 & number 2 & 100 \\ \hline
 finish constructor & finish constructor & 100 \\ \hline
 create method hasnext & create method hasnext & 100 \\ \hline
 return nextnode is not equal to null & return next node is not equal to null & 100 \\ \hline
 we are done with class LLinterrater & we are done with class LLL interrater & 90 \\ \hline
\end{tabular}
\caption{This table represents the context that was dictated and the context that was returned. For each pair there is a matching score [0, 100]\%.}
\label{tab16}
\end{table}
\begin{table}[H]
	\centering
	\begin{tabular}{|p{6cm}|p{6cm}|p{2cm}|}
		\hline
		{\bf Expected Result} & {\bf Actual Result} & {\bf Match Score (\%)} \\ \hline
		create method get that accepts pos of type int & create method get that accepts pose of type int & 100 \\ \hline
		assert pos is between 0 and size & assert pose is between 0 and size & 100 \\ \hline
		if pos equals size -1 & ethos equals size -1 & 80 \\ \hline
		then assign tail. Data to result & then assign tail. Data to result & 100 \\ \hline
		otherwise & otherwise & 100 \\ \hline
		temp is asigned head & temp is a signed head & 100 \\ \hline
		create a loop from one to pos & create a loop from one to pose & 100 \\ \hline
		assign them. next to temp & sign them. Next to temp & 100 \\ \hline
		set the result to temp. data & set result to temp. Data & 100 \\ \hline
		return result & return result & 100 \\ \hline
	\end{tabular}
	\caption{This table is the continuation of \autoref{tab16}.}
	\label{tab17}
\end{table} 
\section{BNF Parser} \label{section:BNF Parser}
\subsubsection{Antlr 4}
Antlr stands for Another Tool for Language Recognition. The tool is able to generate compiler or interpreter for any computer language. In addition to obvious use, e.g. the need to parse a real 'big' programming language such as Java, PHP or SQL, it can help with smaller, more common tasks.

It is useful any time you need to evaluate expressions unknown at compile-time or to parse non-trivial user input or files in a weird format. Of course, it is possible to create a custom handmade parser for any of these tasks. However, it usually takes much more time and effort. A little knowledge of a good parser generator may turn these time-consuming tasks into easy and fast exercises.

ANTLR seems to be popular in open source word. Among others, it is used by Apache Camel, Apache Lucene, Apache Hadoop, Groovy and Hibernate. They all needed parser for a custom language. For example, Hibernate uses ANTLR to parse its query language HQL.

ANTLR is code generator. It takes so called grammar file as input and generates two classes: lexer and parser.

Lexer runs first and splits input into pieces called tokens. Each token represents a more or less meaningful piece of input. The stream of tokes is passed to parser which does all necessary work. It is the parser which builds an abstract syntax tree, interprets the code or translates it into some other form.

Grammar file contains everything ANTLR needs to generate correct lexer and parser Whether it should generate java or python classes, whether parser generates abstract syntax tree, assembler code or directly interprets code and so on.

Most importantly, grammar file describes how to split input into tokens and how to build tree from tokens. In other words, grammar file contains lexer rules and parser rules.

Each lexer rule describes one token: TokenName: regular expression; Parser rules are more complicated. The most basic version is similar as in lexer rule: ParserRuleName: regular expression;  

They may contain modifiers that specify special transformations on input, root and childs in result abstract syntax tree or actions to be performed whenever the rule is used. Almost all work is usually done inside parser rules.

In order to learn how to use Antlr 4,  I used The Definitive ANTLR 4 \cite{Terence2012} book, written by Terence Parr who is the man behind Antlr.
